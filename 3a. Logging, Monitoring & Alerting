We use LogAnalytics Workspaces and Application Insights as our logging service. 


LOGGING:
Service telemetry is sent to LogAnalytics and application level logging is sent to Application Insights
Both logging services provide data to Azure monitor, from where we can run standardised queries, configure alerts and lay the foundation for dashboards

Azure services were configured to log to LogAnalytics and Azure Storage as part of the infrastructure templates

Infrastructure is defined in an ARM template, a sample of the configuration for diagnostics settings are shown below:
{
    "apiVersion": "2017-05-01-preview",
    "type": "Microsoft.DataFactory/factories/providers/diagnosticSettings",
    "name": "[concat(parameters('dataFactoryName'), '/Microsoft.Insights/audit')]",
    "location": "[parameters('location')]",
    "dependsOn": [
        "[resourceId('Microsoft.DataFactory/factories', parameters('dataFactoryName'))]"
    ],
    "properties": {
        "logAnalyticsDestinationType": "Dedicated",
        "workspaceId": "[resourceId('Microsoft.OperationalInsights/workspaces', parameters('logAnalyticsName'))]",
        "metrics": [
            {
                "category": "AllMetrics",
                "enabled": true,
                "retentionPolicy": {
                    "days": 0,
                    "enabled": false
                },
                "timeGrain": null
            }
        ],
        "logs": [
            {
                "category": "ActivityRuns",
                "enabled": true
            },
            {
                "category": "PipelineRuns",
                "enabled": true
            },
            {
                "category": "TriggerRuns",
                "enabled": true
            }
        ]
    }
}
The exact detail of what is logged can be controlled with the metrics and logs properties and by default all information is logged for each service. 
This allows operational teams to identify useful information, then iteratively adjust the infrastructure templates to capture what is necessary. 


Data Pipeline Logging:
Code that is written to run in Databricks uses the opencensus package to redirect logs to Application Insights. 
This is a standard Python package and the recommended interface to Application Insights from Python. 
More details can be found (https://docs.microsoft.com/en-us/azure/azure-monitor/app/opencensus-python).

The following code is an except from this link and shows how this can be implemented in code:

import logging
from opencensus.ext.azure.log_exporter import AzureLogHandler

logger = logging.getLogger(__name__)

logger.addHandler(AzureLogHandler(
    connection_string='InstrumentationKey=00000000-0000-0000-0000-000000000000')
)
# You can also instantiate the exporter directly if you have the environment variable
# `APPLICATIONINSIGHTS_CONNECTION_STRING` configured
# logger.addHandler(AzureLogHandler())

def valuePrompt():
    line = input("Enter a value: ")
    logger.warning(line)

def main():
    while True:
        valuePrompt()

if __name__ == "__main__":
    main()





MONITORING:
Once data has been captured by Azure Monitor, queries can be written to identify significant events. 
These are expressed using KQL (Kusto Query Language), details of which can be found here: (https://docs.microsoft.com/en-us/azure/data-explorer/kql-quick-reference).

The Azure portal offers a web interface to Azure Monitor where KQL queries can be written and executed against logged data. 
This allows ad-hoc queries to be created to analyse different services and pipelines.

The following code shows an example query that identifies any errors arising from Data Factory pipelines:

ADFPipelineRun
| where Status == "Failed"
| project TimeGenerated, DataFactory=split(ResourceId, '/')[8], PipelineName, Status, RunId 
Once this query has been created in Azure Monitor, there is an option available in the GUI to pin the output to a Dashboard. This can either be tabular or a chart

Once a dashboard has been created in the Azure Portal, the ARM template representing the dashboard can be exported, 
parameterised and version controlled to allow it to be rolled out across different environments and pipelines.




ALERTING:
Alerting in EDP is also handled by the Azure platform and Azure Monitor.

Here, action groups can be defined that will be notified when a particular condition is met. 
The following ARM resource shows an action group that is configured to send email addresses to a set of receivers:

{
    "apiVersion": "2018-03-01",
    "type": "Microsoft.Insights/actionGroups",
    "name": "[variables('actionGroupName')]",
    "location": "global",
    "properties": {
        "groupShortName": "[variables('actionGroupName')]",
        "enabled": true,
        "smsReceivers": [],
        "emailReceivers": "[parameters('emailReceivers')]",
        "webhookReceivers": []
    }
}
Once an action group has been defined, a scheduled query rule can be created that will periodically invoke a KQL query against Azure Monitor 

We can then use the results to determine whether to notify an action group or not. The following ARM resource shows a scheduled query rule:

{
    "apiVersion": "2018-04-16",
    "type": "Microsoft.Insights/scheduledQueryRules",
    "name": "[variables('pipelineErrorAlertName')]",
    "location": "[parameters('location')]",
    "dependsOn": [
        "resourceId('Microsoft.Insights/actionGroups', variables('actionGroupName'))]"
    ],
    "properties": {
        "description": "[variables('pipelineErrorAlertDesc')]",
        "enabled": "True",
        "source": {
            "query": "ADFPipelineRun | where Status == \"Failed\" project TimeGenerated, DataFactory=split(ResourceId, '/')[8], PipelineName, Status, RunId",
            "dataSourceId": "[variables('dataSourceId')]",
            "queryType": "ResultCount"
        },
        "schedule": {
            "frequencyInMinutes": 5,
            "timeWindowInMinutes": 5
        },
        "action": {
            "odata.type": "Microsoft.WindowsAzure.Management.Monitoring.Alerts.Models.Microsoft.AppInsights.Nexus.DataContracts.Resources.ScheduledQueryRules.AlertingAction",
            "severity": 4,
            "aznsAction": {
                "actionGroup": "[array(resourceId('Microsoft.Insights/actionGroups', variables('actionGroupName')))]",
                "emailSubject": "Failed Pipelines"
            },
            "trigger": {
                "thresholdOperator": "GreaterThan",
                "threshold": 0
            }
        }
    }
}
The scheduled query rule allows different parameters to be configured:

Name:  Purpose
Query:  The query to be executed. The results returned can be used to trigger an action group.
Frequency In Minutes:  How often to execute the query.
Time Window In Minutes:  How much data to include in the query.
Severity:  The severity of the alert being raised if triggered.
Threshold Operator:  The type of comparison to carry out with the query results.
Threshold:  The threshold before the action group is triggered.

The query above is configured to run every 5 minutes and to look at the preceeding 5 minutes worth of data. 
If the results of the query exceeds the threshold of 0, (i.e., there are errors reported), then the action group will be notified.
